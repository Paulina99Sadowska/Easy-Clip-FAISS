{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOw8yMd1VlnD"
   },
   "source": [
    "# Data Preprocessing Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvUGC8QQV6bV"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import faiss\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfFEXZC0WS-V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ścieżka do folderu ze zdjęciami\n",
    "folder_path = r\"C:\\Users\\user\\Desktop\\CLIP\"\n",
    "\n",
    "# Ładujemy model CLIP (openai/clip-vit-base-patch32)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Funkcja do ekstrakcji wektorów obrazów\n",
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_image_features(**inputs)\n",
    "    return embeddings[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2048132758.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport pandas as pd\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#Tworzenie csv na podstawie zdjęć\n",
    "\n",
    "data = {\n",
    "    \"filename\": [\n",
    "        \"camera.jpeg\",\n",
    "        \"cat.jpeg\",\n",
    "        \"coffee.jpeg\",\n",
    "        \"girl.jpeg\",\n",
    "        \"house_wife.jpeg\",\n",
    "        \"motorcycle_right.jpeg\",\n",
    "        \"page.png\",\n",
    "        \"teacher.jpeg\"\n",
    "    ],\n",
    "    \"label\": [\n",
    "        \"camera\",\n",
    "        \"cat\",\n",
    "        \"coffee\",\n",
    "        \"girl\",\n",
    "        \"house_wife\",\n",
    "        \"motorcycle_right\",\n",
    "        \"page\",\n",
    "        \"teacher\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# zapisujemy do CSV w tym samym katalogu co skrypt\n",
    "df.to_csv(\"Data.csv\", index=False)\n",
    "\n",
    "print(\"Plik Data.csv został utworzony!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqHTg9bxWT_u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znalezione obrazy: ['camera.jpeg', 'cat.jpeg', 'coffee.jpeg', 'girl.jpeg', 'house_wife.jpeg', 'motorcycle_right.jpeg', 'page.png', 'teacher.jpeg']\n",
      "Załadowano 8 obrazów do FAISS.\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Lista plików (tylko obrazy)\n",
    "valid_ext = [\".jpg\", \".jpeg\", \".png\"]\n",
    "image_files = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.splitext(f)[1].lower() in valid_ext\n",
    "]\n",
    "\n",
    "print(\"Znalezione obrazy:\", image_files)\n",
    "\n",
    "# Embeddingi\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    path = os.path.join(folder_path, img_file)\n",
    "    emb = get_image_embedding(path)\n",
    "    embeddings.append(emb)\n",
    "    labels.append(img_file.split(\".\")[0])\n",
    "\n",
    "\n",
    "# Embeddingi\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    path = os.path.join(folder_path, img_file)\n",
    "    emb = get_image_embedding(path)\n",
    "    embeddings.append(emb)\n",
    "    labels.append(img_file.split(\".\")[0])  # np. \"camera\", \"cat\"\n",
    "\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Tworzymy index FAISS\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  # L2 distance\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Załadowano {len(embeddings)} obrazów do FAISS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm48sif-WWsh"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = \"https://images.unsplash.com/photo-1592194996308-7b43878e84a6\"  # przykładowy link do kota\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# zapis do folderu CLIP\n",
    "img.save(r\"C:\\Users\\user\\Desktop\\CLIP\\cat_highres.jpeg\")\n",
    "img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_emb = get_image_embedding(r\"C:\\Users\\user\\Desktop\\CLIP\\cat_highres.jpeg\").astype(\"float32\")\n",
    "index.add(new_emb.reshape(1, -1))\n",
    "labels.append(\"cat_highres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najbliższe dopasowania (bez query samego siebie):\n",
      "cat (distance=66.8705)\n"
     ]
    }
   ],
   "source": [
    "# Ścieżka do obrazu, którego używasz jako query\n",
    "query_image = r\"C:\\Users\\user\\Desktop\\CLIP\\cat_highres.jpeg\"\n",
    "\n",
    "query_emb = get_image_embedding(query_image).astype(\"float32\")\n",
    "D, I = index.search(query_emb.reshape(1, -1), k=10)  # pobieramy więcej wyników\n",
    "\n",
    "print(\"Najbliższe dopasowania (bez query samego siebie):\")\n",
    "for idx, dist in zip(I[0], D[0]):\n",
    "    if labels[idx] != \"cat_highres\":  # ignorujemy obraz użyty jako query\n",
    "        print(f\"{labels[idx]} (distance={dist:.4f})\")\n",
    "        break  # jeśli chcemy tylko 1 najlepszy wynik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\user\\Desktop\\CLIP\"  # folder z Twoimi obrazami\n",
    "valid_ext = [\".jpg\", \".jpeg\", \".png\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_image_embedding_from_pil(image_pil):\n",
    "    inputs = processor(images=image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "    return emb[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Wczytaj obrazy i stwórz embeddingi\n",
    "# -----------------------------\n",
    "image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in valid_ext]\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for img_file in image_files:\n",
    "    path = os.path.join(folder_path, img_file)\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    emb = get_image_embedding_from_pil(img)\n",
    "    embeddings.append(emb)\n",
    "    labels.append(img_file.split(\".\")[0])\n",
    "\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano 8 obrazów do FAISS.\n"
     ]
    }
   ],
   "source": [
    "# 4️⃣ Tworzymy indeks FAISS\n",
    "# -----------------------------\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Załadowano {len(embeddings)} obrazów do FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_image(img: Image.Image):\n",
    "    # embedding zapytania\n",
    "    query_emb = get_image_embedding_from_pil(img).astype(\"float32\")\n",
    "    \n",
    "    # top-3 podobne obrazy\n",
    "    D, I = index.search(query_emb.reshape(1, -1), k=3)\n",
    "    \n",
    "    # przygotowanie wyników do Gradio (miniaturki + etykieta)\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        result_img = Image.open(os.path.join(folder_path, image_files[idx]))\n",
    "        results.append((result_img, labels[idx]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6️⃣ Interfejs Gradio\n",
    "# -----------------------------\n",
    "iface = gr.Interface(\n",
    "    fn=recognize_image,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=gr.Gallery(label=\"Najbliższe obrazy\"),\n",
    "    title=\"CLIP + FAISS Prosta wyszukiwarka\"\n",
    "    description=\"Prześlij obraz, a model znajdzie podobne obrazy w bazie.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOD2/gZgY69JdiiGJVNfu7s",
   "collapsed_sections": [],
   "name": "data_preprocessing_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
